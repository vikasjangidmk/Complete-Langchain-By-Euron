{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"You are a helpful assistant. Write a short poem about {topic}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI \n",
    "# âœ… Step 2: Initialize an LLM (OpenAI GPT)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableSequence(first=prompt, middle=[llm], last=parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RunnableSequence Output]\n",
      " In the vast expanse beyond our skies,\n",
      "Adventurers boldly reach new highs.\n",
      "Through galaxies and nebulae they roam,\n",
      "Exploring the great unknown.\n",
      "\n",
      "They navigate the cosmic sea,\n",
      "Seeking out what lies unseen.\n",
      "From distant planets to shining stars,\n",
      "They journey on to lands afar.\n",
      "\n",
      "With courage and curiosity as their guide,\n",
      "They push the boundaries far and wide.\n",
      "In search of answers to questions profound,\n",
      "They explore the mysteries that abound.\n",
      "\n",
      "So let us cheer for those who dare,\n",
      "To venture out into the cosmic air.\n",
      "For in their quest to understand,\n",
      "They pave the way for future explorers to expand.\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\"topic\": \"space exploration\"})\n",
    "print(\"\\n[RunnableSequence Output]\\n\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "chain=RunnablePassthrough()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnablePassthrough\n",
    "* It does not do anything to the input data.\n",
    "* Let's see it in a very simple example: a chain with just RunnablePassthrough() will output the original input without any modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "chain=RunnablePassthrough()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, world!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence, RunnablePassthrough\n",
    "\n",
    "passthrough1 = RunnablePassthrough()\n",
    "passthrough2 = RunnablePassthrough()\n",
    "\n",
    "chain = RunnableSequence(first=passthrough1, last=passthrough2)\n",
    "\n",
    "print(chain.invoke(\"Hello world\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableLambda\n",
    "* To use a custom function inside a LCEL chain we need to wrap it up with RunnableLambda.\n",
    "* Let's define a very simple function to create Russian lastnames:## RunnableLambda\n",
    "* To use a custom function inside a LCEL chain we need to wrap it up with RunnableLambda.\n",
    "* Let's define a very simple function to create Russian lastnames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def russian_lastname(name: str) -> str:\n",
    "    return f\"{name}ovich\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnablePassthrough() | RunnableLambda(russian_lastname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vikas ovich'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Vikas \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableMap\n",
    "\n",
    "stripper = RunnableLambda(lambda x: x.strip())\n",
    "lowercase = RunnableLambda(lambda x: x.lower())\n",
    "length = RunnableLambda(lambda x: len(x))\n",
    "\n",
    "\n",
    "parallel = RunnableMap({\n",
    "    \"cleaned\": stripper,\n",
    "    \"lowered\": lowercase,\n",
    "    \"length\": length\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cleaned': 'LangChain Rocks!', 'lowered': '   langchain rocks!   ', 'length': 22}\n"
     ]
    }
   ],
   "source": [
    "result = parallel.invoke(\"   LangChain Rocks!   \")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableParallel\n",
    "* We will use RunnableParallel() for running tasks in parallel.\n",
    "* This is probably the most important and most useful Runnable from LangChain.\n",
    "* In the following chain, RunnableParallel is going to run these two tasks in parallel:\n",
    "    * operation_a will use RunnablePassthrough.\n",
    "    * operation_b will use RunnableLambda with the russian_lastname function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\": RunnablePassthrough(),\n",
    "        \"operation_b\": RunnableLambda(russian_lastname)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'operation_a': 'prince', 'operation_b': 'princeovich'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"prince\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a curious fact about {soccer_player}\")\n",
    "\n",
    "output_parser = StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def russian_lastname_from_dictionary(person):\n",
    "    return person[\"name\"] + \"ovich\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\": RunnablePassthrough(),\n",
    "        \"soccer_player\": RunnableLambda(russian_lastname_from_dictionary),\n",
    "        \"operation_c\": RunnablePassthrough(),\n",
    "    }\n",
    ") | prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There is no official player named \"Cristiano Ronaldoovich.\" Cristiano Ronaldo, whose full name is Cristiano Ronaldo dos Santos Aveiro, is a Portuguese professional footballer widely regarded as one of the greatest players of all time.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"soccer_player\": \"Cristiano Ronaldo\", \"name\": \"Cristiano Ronaldo\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Princeovich is not a real surname, but rather a title commonly used in European monarchies to indicate a prince of a particular region or country.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"name\": \"prince\", \"soccer_player\": \"messi\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RunnableBranch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling calculator tool...\n",
      "Running text summarizer...\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableBranch, RunnableLambda\n",
    "\n",
    "\n",
    "math_handler = RunnableLambda(lambda x: \"Calling calculator tool...\")\n",
    "text_handler = RunnableLambda(lambda x: \"Running text summarizer...\")\n",
    "default_handler = RunnableLambda(lambda x: f\"No match, fallback: {x}\")\n",
    "\n",
    "\n",
    "router = RunnableBranch(\n",
    "    (lambda x: \"math\" in x.lower(), math_handler),\n",
    "    (lambda x: \"text\" in x.lower(), text_handler),\n",
    "    default_handler\n",
    ")\n",
    "\n",
    "\n",
    "result = router.invoke(\"math\")\n",
    "print(result)  # Output: Calling calculator tool...\n",
    "result = router.invoke(\"text\")\n",
    "print(result)  # Output: Running text summarizer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = PromptTemplate.from_template(\"Answer the question: {query}\")\n",
    "qa_chain = qa_prompt | ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Why did the AI break up with his girlfriend? Because he couldn't handle her emotional bandwidth!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 14, 'total_tokens': 33, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bc44aTv0cdnNqRWvpE12SmMUNHMM2', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--1b16ee7a-2b7f-4706-a3b0-6f686b8c1b7a-0' usage_metadata={'input_tokens': 14, 'output_tokens': 19, 'total_tokens': 33, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "joke_prompt = PromptTemplate.from_template(\"Tell a joke about {query}\")\n",
    "joke_chain = joke_prompt | ChatOpenAI()\n",
    "\n",
    "\n",
    "router = RunnableBranch(\n",
    "    (lambda x: \"joke\" in x[\"query\"], joke_chain),\n",
    "    qa_chain  # default: answer question\n",
    ")\n",
    "\n",
    "\n",
    "result = router.invoke({\"query\": \"joke about AI\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The End\n"
     ]
    }
   ],
   "source": [
    "print(\"The End\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
