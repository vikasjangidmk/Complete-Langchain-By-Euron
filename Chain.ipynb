{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f150529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fef00351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1057a2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikas\\AppData\\Local\\Temp\\ipykernel_24204\\341822862.py:9: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llmchain = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=\"what is the capital of {input}?\"\n",
    ")\n",
    "\n",
    "llmchain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "638370a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikas\\AppData\\Local\\Temp\\ipykernel_24204\\1496948778.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  llmchain.run(\"France\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mwhat is the capital of France?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmchain.run(\"France\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60973b56",
   "metadata": {},
   "source": [
    "# **SimpleSequentialChain and SequentialChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f790eee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain, SequentialChain\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55a3accb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = PromptTemplate(\n",
    "    input_variables=[\"cuisine\"],\n",
    "    template=\"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "\n",
    "name_chain = LLMChain(llm=llm, prompt = prompt1, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27750373",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = PromptTemplate(\n",
    "    input_variables=[\"restaurant_name\"],\n",
    "    template=\"Suggest some menu item for {restaurant_name} restaurant?\"\n",
    ")\n",
    "\n",
    "food_item_chain = LLMChain(llm=llm, prompt=prompt2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "914c43c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI want to open a restaurant for Italian food. Suggest a fency name for this.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\"Bellissimo Trattoria\"\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSuggest some menu item for \"Bellissimo Trattoria\" restaurant?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m1. Antipasto Platter - a selection of cured meats, cheeses, olives, and marinated vegetables\n",
      "2. Homemade Gnocchi with Pesto Sauce\n",
      "3. Veal Scaloppine with Lemon Caper Sauce\n",
      "4. Seafood Risotto with Shrimp, Scallops, and Mussels\n",
      "5. Eggplant Parmigiana\n",
      "6. Tiramisu for dessert\n",
      "7. Linguine with Clams in White Wine Garlic Sauce\n",
      "8. Caprese Salad with Fresh Mozzarella, Tomatoes, and Basil\n",
      "9. Chicken Marsala with Mushrooms\n",
      "10. Cannoli for dessert\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "1. Antipasto Platter - a selection of cured meats, cheeses, olives, and marinated vegetables\n",
      "2. Homemade Gnocchi with Pesto Sauce\n",
      "3. Veal Scaloppine with Lemon Caper Sauce\n",
      "4. Seafood Risotto with Shrimp, Scallops, and Mussels\n",
      "5. Eggplant Parmigiana\n",
      "6. Tiramisu for dessert\n",
      "7. Linguine with Clams in White Wine Garlic Sauce\n",
      "8. Caprese Salad with Fresh Mozzarella, Tomatoes, and Basil\n",
      "9. Chicken Marsala with Mushrooms\n",
      "10. Cannoli for dessert\n"
     ]
    }
   ],
   "source": [
    "chain = SimpleSequentialChain(chains=[name_chain, food_item_chain], verbose=True)\n",
    "result = chain.run(\"Italian\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2648cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = PromptTemplate(\n",
    "    input_variables=[\"cuisine\"],\n",
    "    template=\"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "name_chain = LLMChain(llm=llm, prompt = prompt1,output_key = \"restaurant_name\" ,verbose = True)\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=[\"restaurant_name\"],\n",
    "    template=\"Suggest some menu item for {restaurant_name} restaurant?\"\n",
    ")\n",
    "food_item_chain = LLMChain(llm=llm, prompt=prompt2, output_key = \"menu_items\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2422be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8a461aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = SequentialChain(\n",
    "    chains = [name_chain, food_item_chain],\n",
    "    input_variables=[\"cuisine\"],\n",
    "    output_variables=[\"restaurant_name\", \"menu_items\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef5d478b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikas\\AppData\\Local\\Temp\\ipykernel_24204\\2231638557.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(chain({\"cuisine\": \"Italian\"}))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI want to open a restaurant for Italian food. Suggest a fency name for this.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSuggest some menu item for Bella Cucina Trattoria restaurant?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'cuisine': 'Italian', 'restaurant_name': 'Bella Cucina Trattoria', 'menu_items': '1. Antipasto platter: A selection of cured meats, cheeses, olives, and marinated vegetables.\\n2. Caprese salad: Fresh tomatoes, mozzarella, basil, and balsamic glaze.\\n3. Pasta alla carbonara: Spaghetti with pancetta, egg, Parmesan cheese, and black pepper.\\n4. Chicken piccata: Pan-seared chicken breast in a lemon butter caper sauce.\\n5. Eggplant Parmigiana: Breaded and fried eggplant slices topped with marinara sauce and melted cheese.\\n6. Risotto ai funghi: Creamy risotto with mixed mushrooms, Parmesan cheese, and fresh herbs.\\n7. Tiramisu: Classic Italian dessert made with layers of coffee-soaked ladyfingers and mascarpone cheese.\\n8. Espresso: A strong, rich coffee to finish off your meal.'}\n"
     ]
    }
   ],
   "source": [
    "print(chain({\"cuisine\": \"Italian\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a24a147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story: In the year 2050, humanity had finally achieved the technology necessary to explore the vast reaches of space. A team of astronauts was selected to embark on a mission to explore a distant planet that showed signs of potentially supporting life.\n",
      "\n",
      "As the spaceship launched into the unknown, the team felt a mix of excitement and apprehension. They knew that they were venturing into uncharted territory, where anything could happen. But they were driven by their curiosity and desire to uncover the mysteries of the universe.\n",
      "\n",
      "After several weeks of travel, the team finally arrived at the planet. They marveled at its alien landscape, with towering mountains and vast oceans. As they began to explore, they discovered strange plant life and creatures unlike anything they had ever seen.\n",
      "\n",
      "But as they delved deeper into the planet's secrets, they encountered unexpected challenges. A sudden storm struck, threatening to destroy their ship. And as they searched for shelter, they stumbled upon a hidden civilization of intelligent beings who were wary of their presence.\n",
      "\n",
      "Despite the obstacles they faced, the team persevered, determined to learn all they could about this fascinating planet. They made contact with the alien inhabitants, who shared their knowledge and technology with the astronauts. And as they worked together to overcome the challenges of their new environment, they forged a bond that transcended language and culture.\n",
      "\n",
      "In the end, the team returned to Earth with a wealth of knowledge and experiences that would forever change humanity's understanding of the universe. Their journey had been a testament to the power of exploration and discovery, and a reminder of the endless possibilities that awaited those brave enough to venture into the unknown depths of space.\n",
      "Analysis: The main theme of this story is exploration and discovery. The astronauts are driven by their curiosity and desire to uncover the mysteries of the universe, which leads them to embark on a mission to explore a distant planet. Despite facing unexpected challenges, they persevere and ultimately succeed in learning about the planet and its inhabitants. The story highlights the importance of pushing boundaries, taking risks, and embracing the unknown in order to expand knowledge and understanding.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "# First chain: Write a story\n",
    "story_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Write a short story about {topic}.\"\n",
    ")\n",
    "story_chain = LLMChain(llm=llm, prompt=story_prompt, output_key=\"story\")\n",
    "\n",
    "# Second chain: Analyze that story\n",
    "analysis_prompt = PromptTemplate(\n",
    "    input_variables=[\"story\"],\n",
    "    template=\"Analyze the following story and identify its main theme: {story}\"\n",
    ")\n",
    "analysis_chain = LLMChain(llm=llm, prompt=analysis_prompt, output_key=\"analysis\")\n",
    "\n",
    "# Combine both\n",
    "sequential_chain = SequentialChain(\n",
    "    chains=[story_chain, analysis_chain],\n",
    "    input_variables=[\"topic\"],\n",
    "    output_variables=[\"story\", \"analysis\"]\n",
    ")\n",
    "\n",
    "result = sequential_chain({\"topic\": \"space exploration\"})\n",
    "print(\"Story:\", result[\"story\"])\n",
    "print(\"Analysis:\", result[\"analysis\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaeb81d",
   "metadata": {},
   "source": [
    "# **Memory Based Chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "664afba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7844ad0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikas\\AppData\\Local\\Temp\\ipykernel_24204\\406878826.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "C:\\Users\\vikas\\AppData\\Local\\Temp\\ipykernel_24204\\406878826.py:5: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation_chain = ConversationChain(llm = llm, memory=memory, verbose=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello, My name is Vikas Jangid\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello, My name is Vikas Jangid\n",
      "AI: Hello Vikas Jangid! It's nice to meet you. How can I assist you today?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Your name is Vikas Jangid.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation_chain = ConversationChain(llm = llm, memory=memory, verbose=True)\n",
    "\n",
    "conversation_chain.run(input = \"Hello, My name is Vikas Jangid\")\n",
    "\n",
    "response = conversation_chain.run(input=\"What is my name?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036b7ed0",
   "metadata": {},
   "source": [
    "# **ConversationBufferWindowMemory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "519cb64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikas\\AppData\\Local\\Temp\\ipykernel_24204\\2361996400.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=3, return_messages=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[]\n",
      "Human: Hi, my name is John.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, my name is John.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello John! It's nice to meet you. How can I assist you today?\", additional_kwargs={}, response_metadata={})]\n",
      "Human: I live in New York.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, my name is John.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello John! It's nice to meet you. How can I assist you today?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='I live in New York.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's great to know! New York is a vibrant city with so much to offer. Are you enjoying living there?\", additional_kwargs={}, response_metadata={})]\n",
      "Human: My favorite color is blue.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, my name is John.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello John! It's nice to meet you. How can I assist you today?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='I live in New York.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's great to know! New York is a vibrant city with so much to offer. Are you enjoying living there?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='My favorite color is blue.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Blue is a wonderful choice for a favorite color! It's often associated with calmness and tranquility. Do you have a specific shade of blue that you prefer, like sky blue or navy blue?\", additional_kwargs={}, response_metadata={})]\n",
      "Human: Where am I live?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response: You live in New York! It's a vibrant city with so much to offer. Is there anything specific you'd like to know about living in New York?\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=3, return_messages=True)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "conversation_chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "conversation_chain.run(\"Hi, my name is John.\")\n",
    "conversation_chain.run(\"I live in New York.\")\n",
    "conversation_chain.run(\"My favorite color is blue.\")\n",
    "\n",
    "\n",
    "response = conversation_chain.run(\"Where am I live?\")\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12301340",
   "metadata": {},
   "source": [
    "# **RetrievalQA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29f7d033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: LangChain is a framework for developing applications powered by language models. It provides modules for working with language models.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "docs = [\n",
    "    \"LangChain is a framework for developing applications powered by language models.\",\n",
    "    \"It enables applications that are context-aware and can reason.\",\n",
    "    \"LangChain provides modules for working with language models.\"\n",
    "]\n",
    "\n",
    "vectorstore = FAISS.from_texts(docs, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "\n",
    "retrieval_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "query = \"What is LangChain?\"\n",
    "response = retrieval_chain.run(query)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216e7941",
   "metadata": {},
   "source": [
    "# **Extraction Chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b753d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikas\\AppData\\Local\\Temp\\ipykernel_24204\\2179365055.py:14: LangChainDeprecationWarning: LangChain has introduced a method called `with_structured_output` thatis available on ChatModels capable of tool calling.You can read more about the method here: <https://python.langchain.com/docs/modules/model_io/chat/structured_output/>. Please follow our extraction use case documentation for more guidelineson how to do information extraction with LLMs.<https://python.langchain.com/docs/use_cases/extraction/>. If you notice other issues, please provide feedback here:<https://github.com/langchain-ai/langchain/discussions/18154>\n",
      "  extraction_chain = create_extraction_chain( llm = llm , schema = schema, verbose = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: Extract and save the relevant entities mentioned in the following passage together with their properties.\n",
      "\n",
      "Only extract the properties mentioned in the 'information_extraction' function.\n",
      "\n",
      "If a property is not present and is not required in the function parameters, do not include it in the output.\n",
      "\n",
      "Passage:\n",
      "prince is 30 years old and enjoys reading and swimming.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response: [{'name': 'prince', 'age': 30, 'hobbies': ['reading', 'swimming']}]\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_extraction_chain\n",
    "llm = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "\n",
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"name\": {\"type\": \"string\"},\n",
    "        \"age\": {\"type\": \"integer\"},\n",
    "        \"hobbies\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "    },\n",
    "    \"required\": [\"name\", \"age\"]\n",
    "}\n",
    "\n",
    "extraction_chain = create_extraction_chain( llm = llm , schema = schema, verbose = True)\n",
    "\n",
    "\n",
    "text = \"prince is 30 years old and enjoys reading and swimming.\"\n",
    "response=extraction_chain.run(text)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cb72cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The End\n"
     ]
    }
   ],
   "source": [
    "print(\"The End\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
